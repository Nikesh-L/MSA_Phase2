{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nikesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#window will open -> models -> punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-453c0664ee5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# only bigrams that appear 2+ times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(sample)\n",
    "\n",
    "# only bigrams that appear 2+ times\n",
    "finder.apply_freq_filter(2) \n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print (finder.nbest(bigram_measures.pmi, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_bigrams(tokenized_text, min_freq, top_n):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokenized_text)\n",
    "    finder.apply_freq_filter(min_freq) \n",
    "    finder.nbest(bigram_measures.pmi, top_n)\n",
    "    return finder.nbest(bigram_measures.pmi, top_n)\n",
    "\n",
    "print(common_bigrams(all_text, 10, 10)) # top 10 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install wordcloud into the current path\n",
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = comments.iloc[:10000,:].Reply.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any rows with empty values in the text column\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "#check the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the stopwords package\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation \n",
    "data.text = data.text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords from the text which cannot provide any information since we cannot infer their sentiment\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "newStopWords = ['im', 'the', 'got', 'go', 'want', 'oh', 'week', 'hour', 'see', 'still', 'say', 'today', 'day', 'going',\n",
    "               'one', 'right', 'twitter', 'tomorrow']\n",
    "stopwords.extend(newStopWords)\n",
    "data.text = data.text.apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.lower() not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any supplied links to text columns\n",
    "data.text = data.text.apply(lambda x: re.sub(r'\\(?http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unicode\n",
    "data.text = data.text.apply(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any escape sequences\n",
    "data.text = data.text.apply(lambda x: re.sub(r'[\\n\\r\\t]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove starting and traling whitespace characters\n",
    "data.text = data.text.apply(lambda x: re.sub(r'^[ \\t]+|[ \\t]+$', '', x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are removing words from rows we shoudl check again for any \n",
    "#new rows that soleely consisted of links or stopwords and are now empty\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any rows with empty values in the text column\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "#check the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization to find the root forms of words.\n",
    "Lemmatization is favoured over stemming since since stemming may lead to creating non-existent words.\n",
    "Lemmatization is slower but it has a 'dictionary-based' approach and we have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required package\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "#For our dataframe we first tokenize, apply lemmatization then join the spaces\n",
    "data.text = data.text.apply(lambda x: ' '.join(\n",
    "    [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]\n",
    "        ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use bigrams to add stopwords for our wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_string = data.iloc[:10000,:].text.str.cat(sep='. ')\n",
    "print(common_bigrams(data_string, 20, 20)) # top 10 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_string = data.iloc[:10000,:].text.str.cat(sep='. ')\n",
    "\n",
    "stp = STOPWORDS.copy()\n",
    "stp.add('Im') #add stopwords to remove from the plot\n",
    "wc = WordCloud(background_color=\"white\", max_words=200,  stopwords=stp)\n",
    "# generate word cloud\n",
    "wc.generate(data_string)\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text.head(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer to help us create a matrix for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "#vectorizer = TfidfVectorizer()\n",
    "data.dropna(subset=['text'], inplace=True)\n",
    "X = vectorizer.fit_transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a validation set\n",
    "train_x, val_x, train_y, val_y = train_test_split(X,data.sentiment,test_size=0.2,random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a multinomial naive bayes model, then fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "#nb.fit(features_matrix, item_we_want_to_predict)\n",
    "#nb.fit(X, data.sentiment)\n",
    "nb.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = nb.predict(val_x)\n",
    "accuracy = metrics.accuracy_score(predicted,val_y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign every prediction to the sentiment column of the submission csv we must:\n",
    "1. load in the test and submission CSV files\n",
    "2. Perform the exact same preprocessing as we did for the training set\n",
    "3. Predict the sentiment for the whole test set and put them in the corresponding row in the submission CSV\n",
    " ### Assumption that the rows are lined up in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we remove the rows with empty text cells\n",
    "testing.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "#remove punctuation \n",
    "data.text = data.text.apply(lambda x: re.sub(r'[^\\w\\s]', '', x) )\n",
    "\n",
    "#remove stopwords\n",
    "data.text = data.text.apply(lambda x: ' '.join([word for word in nltk.word_tokenize(x) if word.lower() not in stopwords]))\n",
    "\n",
    "#remove any supplied links to text columns\n",
    "data.text = data.text.apply(lambda x: re.sub(r'\\(?http\\S+', '', x))\n",
    "\n",
    "#remove unicode\n",
    "data.text = data.text.apply(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "#remove any starting or trailing whitespace\n",
    "data.text = data.text.apply(lambda x: re.sub(r'^[ \\t]+|[ \\t]+$', '', x))\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "testing.text = testing.text.apply(lambda x: ' '.join(\n",
    "    [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]\n",
    "        ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorise our text\n",
    "test_vect = vectorizer.transform(testing.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the predictions from the test set\n",
    "test_predict = nb.predict(test_vect)\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the predictions to the submission file under the sentiment column\n",
    "submission.sentiment = test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the submission file to a csv\n",
    "#submission.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare differences between package and the trained model from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_vect = vectorizer.transform(df.Reply.values.astype('U'))\n",
    "comment_sentiment = nb.predict(comments_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = comment_sentiment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great = df[df.sentiment.str.contains(\"positive\")]\n",
    "great.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df.sentiment.str.contains(\"positive\")].loc[df['compound'] <= 0.0]\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_acc = (great.shape[0]-pos.shape[0])/great.shape[0]\n",
    "pos_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great = df[df.sentiment.str.contains(\"negative\")]\n",
    "great.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = df[df.sentiment.str.contains(\"negative\")].loc[df['compound'] >= 0.0]\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_acc = (great.shape[0]-neg.shape[0])/great.shape[0]\n",
    "neg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "great = df[df.sentiment.str.contains(\"neutral\")]\n",
    "great.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = df[df.sentiment.str.contains(\"neutral\")].loc[df['compound'] != 0.0]\n",
    "neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_acc = (great.shape[0]-neu.shape[0])/great.shape[0]\n",
    "neu_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
