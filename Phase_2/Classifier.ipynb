{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading nltk: Package 'nltk' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#window will open -> models -> punkt\n",
    "nltk.download('nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the comments csv that we got from the reddit API\n",
    "comments = pd.read_csv('comments_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for everything.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = comments.Reply.iloc[35]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks for everything.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\(?http\\S+', '', sample)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Salut  Comment ca va ?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove escape characters\n",
    "s = \"Salut \\n Comment ca va ?\"\n",
    "s = re.sub(r'[\\n\\r\\t]', '', s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       qwkbbqheqb '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove unicode\n",
    "uni = 'ï ï ï ïqwkbbqheqbï'\n",
    "uni = re.sub(r'[^\\x00-\\x7F]+',' ', uni)\n",
    "uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwkbbqheqb'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove trailing whietspace after whitespace characters\n",
    "text = '       qwkbbqheqb '\n",
    "text = re.sub(r'^[ \\t]+|[ \\t]+$','', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reply</th>\n",
       "      <th>Upvote</th>\n",
       "      <th>Time</th>\n",
       "      <th>Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MY FUCKING LEGEND\\n\\nALWAYS A BLUE</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2019-05-30 08:59:32</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We moved from the old guard era to the Eden ha...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019-05-30 09:32:57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>One of the most skilful and talented players w...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2019-05-30 09:28:08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>First trophy in Chelsea was EL, last trophy al...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-05-30 09:24:43</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>As a belgian I'm so proud of this guy, he does...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-05-30 12:16:52</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48691</th>\n",
       "      <td>&gt;You make a good point about the difference be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-05-30 10:26:22</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48693</th>\n",
       "      <td>Those are official accounts with social media ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-05-30 05:59:14</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48695</th>\n",
       "      <td>You okay bro? You just replied to like 6 of my...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-05-30 06:15:48</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48696</th>\n",
       "      <td>Xenophobic, whatever you want to call it.\\n\\nI...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-05-30 22:59:09</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48699</th>\n",
       "      <td>Lol, you're literally insane\\n\\nAlso, are you ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-05-30 07:24:18</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6595 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Reply  Upvote  \\\n",
       "5                     MY FUCKING LEGEND\\n\\nALWAYS A BLUE    63.0   \n",
       "12     We moved from the old guard era to the Eden ha...    12.0   \n",
       "19     One of the most skilful and talented players w...     7.0   \n",
       "20     First trophy in Chelsea was EL, last trophy al...     6.0   \n",
       "29     As a belgian I'm so proud of this guy, he does...     5.0   \n",
       "...                                                  ...     ...   \n",
       "48691  >You make a good point about the difference be...     1.0   \n",
       "48693  Those are official accounts with social media ...     0.0   \n",
       "48695  You okay bro? You just replied to like 6 of my...     0.0   \n",
       "48696  Xenophobic, whatever you want to call it.\\n\\nI...     2.0   \n",
       "48699  Lol, you're literally insane\\n\\nAlso, are you ...     1.0   \n",
       "\n",
       "                      Time    Key  \n",
       "5      2019-05-30 08:59:32    1.0  \n",
       "12     2019-05-30 09:32:57    1.0  \n",
       "19     2019-05-30 09:28:08    1.0  \n",
       "20     2019-05-30 09:24:43    1.0  \n",
       "29     2019-05-30 12:16:52    1.0  \n",
       "...                    ...    ...  \n",
       "48691  2020-05-30 10:26:22  499.0  \n",
       "48693  2020-05-30 05:59:14  499.0  \n",
       "48695  2020-05-30 06:15:48  499.0  \n",
       "48696  2020-05-30 22:59:09  499.0  \n",
       "48699  2020-05-30 07:24:18  499.0  \n",
       "\n",
       "[6595 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "great = comments[comments.Reply.str.contains(\"\\n\")]\n",
    "great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.Reply = comments.Reply.apply(lambda x: re.sub(r'[\\n\\r\\t]', '', x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reply</th>\n",
       "      <th>Upvote</th>\n",
       "      <th>Time</th>\n",
       "      <th>Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Reply, Upvote, Time, Key]\n",
       "Index: []"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that we have successfully removed escape characters\n",
    "great = comments[comments.Reply.str.contains(\"\\n\")]\n",
    "great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thanks', 'for', 'everything', '.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenized = nltk.word_tokenize(sample)\n",
    "word_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['more', 'is', 'said', 'than', 'done', 'is', 'said', 'than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 'said'), ('said', 'than')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(sample)\n",
    "\n",
    "# only bigrams that appear 2+ times\n",
    "finder.apply_freq_filter(2) \n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print (finder.nbest(bigram_measures.pmi, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect all the first 10000 comments in the reply column into a single line separated by a period\n",
    "all_text = comments.iloc[:10000,:].Reply.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = nltk.word_tokenize(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-02d5b4d3d0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# top 10 bigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-02d5b4d3d0c7>\u001b[0m in \u001b[0;36mcommon_bigrams\u001b[0;34m(tokenized_text, min_freq, top_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommon_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbigram_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/collocations.py\u001b[0m in \u001b[0;36mfrom_words\u001b[0;34m(cls, words, window_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specify window_size at least 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/util.py\u001b[0m in \u001b[0;36mngrams\u001b[0;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def common_bigrams(tokenized_text, min_freq, top_n):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokenized_text)\n",
    "    finder.apply_freq_filter(min_freq) \n",
    "    finder.nbest(bigram_measures.pmi, top_n)\n",
    "    return finder.nbest(bigram_measures.pmi, top_n)\n",
    "\n",
    "print(common_bigrams(all_text, 10, 10)) # top 10 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install wordcloud into the current path\n",
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = comments.iloc[:10000,:].Reply.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp = STOPWORDS.copy()\n",
    "#stp.add() #add stopwords to remove from the plot\n",
    "wc = WordCloud(background_color=\"white\", max_words=50,  stopwords=stp)\n",
    "# generate word cloud\n",
    "wc.generate(sample)\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader Sentitment Analysis is good for analysing text posts from social media, which I'm making the assumption since how popular Reddit has become. It has become more than just a discussion forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had an error downlaoding the VaderConstants Package and omitted it from the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#FIRST, we initialize VADER so we can use it within our Python script\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually change the sentiment attributed to certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {\n",
    "    \n",
    "}\n",
    "sid.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subset of the comments from the first 50 posts\n",
    "df = comments\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cleaning steps as outlined earlier\n",
    "df.Reply = df.Reply.apply(lambda x: re.sub(r'[^\\w\\s]', '', x) )\n",
    "df.Reply = df.Reply.apply(lambda x: ' '.join(nltk.word_tokenize(x)))\n",
    "df.Reply = df.Reply.apply(lambda x: re.sub(r'\\(?http\\S+', '', x))\n",
    "df.Reply = df.Reply.apply(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "df.Reply = df.Reply.apply(lambda x: re.sub(r'^[ \\t]+|[ \\t]+$', '', x))\n",
    "df.Reply = df.Reply.apply(lambda x: ' '.join(\n",
    "    [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(x)]\n",
    "        ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['neg','neu','pos','compound']] = df.Reply.apply(lambda x: pd.Series(sid.polarity_scores(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
